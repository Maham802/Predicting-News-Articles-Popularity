{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "aPh1eEfV7xvg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the development set\n",
        "devp_df = pd.read_csv('development.csv')\n",
        "\n",
        "# Load the evaluation set\n",
        "eval_df = pd.read_csv('evaluation.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBI1fX2H7xvl"
      },
      "source": [
        "# 1. Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwJjo1M07xvo",
        "outputId": "9bc6b355-f0fd-4af3-ec8c-805d2810d49b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(31715, 50)\n"
          ]
        }
      ],
      "source": [
        "# Check the data structure and dimensions\n",
        "#devp_df = devp_df.drop_duplicates()\n",
        "print(devp_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJrID6xR7xv5",
        "outputId": "fec5eb7f-037f-4239-d2ff-3c00f15dcfd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id                                                url  timedelta  \\\n",
            "0   0  http://mashable.com/2014/09/08/safest-cabbies-...      121.0   \n",
            "1   1   http://mashable.com/2013/07/25/3d-printed-rifle/      532.0   \n",
            "2   2  http://mashable.com/2013/10/30/digital-dinosau...      435.0   \n",
            "3   3  http://mashable.com/2014/08/27/homer-simpson-i...      134.0   \n",
            "4   4  http://mashable.com/2013/01/10/creepy-robotic-...      728.0   \n",
            "\n",
            "   n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
            "0            12.0            1015.0         0.422018               1.0   \n",
            "1             9.0             503.0         0.569697               1.0   \n",
            "2             9.0             232.0         0.646018               1.0   \n",
            "3            12.0             171.0         0.722892               1.0   \n",
            "4            11.0             286.0         0.652632               1.0   \n",
            "\n",
            "   n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  ...  \\\n",
            "0                  0.545031       10.0             6.0  ...   \n",
            "1                  0.737542        9.0             0.0  ...   \n",
            "2                  0.748428       12.0             3.0  ...   \n",
            "3                  0.867925        9.0             5.0  ...   \n",
            "4                  0.800000        5.0             2.0  ...   \n",
            "\n",
            "   avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
            "0              -0.160714                  -0.50              -0.071429   \n",
            "1              -0.157500                  -0.25              -0.100000   \n",
            "2              -0.427500                  -1.00              -0.187500   \n",
            "3              -0.216667                  -0.25              -0.166667   \n",
            "4              -0.251786                  -0.50              -0.100000   \n",
            "\n",
            "   title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
            "0                 0.0                      0.00                     0.5   \n",
            "1                 0.0                      0.00                     0.5   \n",
            "2                 0.0                      0.00                     0.5   \n",
            "3                 0.4                     -0.25                     0.1   \n",
            "4                 0.2                     -0.10                     0.3   \n",
            "\n",
            "   abs_title_sentiment_polarity  shares  data_channel    weekday  \n",
            "0                          0.00    2900           bus    tuesday  \n",
            "1                          0.00    1300          tech   thursday  \n",
            "2                          0.00   17700     lifestyle  wednesday  \n",
            "3                          0.25    1500           bus  wednesday  \n",
            "4                          0.10    1400          tech   thursday  \n",
            "\n",
            "[5 rows x 50 columns]\n"
          ]
        }
      ],
      "source": [
        "print(devp_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcfZubu97xv6",
        "outputId": "5090a858-c285-47d8-a4c3-76d24baec07a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                id     timedelta  n_tokens_title  n_tokens_content  \\\n",
            "count  31715.00000  31715.000000    31715.000000      31715.000000   \n",
            "mean   15857.00000    354.058206       10.390730        544.048179   \n",
            "std     9155.47623    214.314223        2.115643        467.730448   \n",
            "min        0.00000      8.000000        2.000000          0.000000   \n",
            "25%     7928.50000    163.000000        9.000000        246.000000   \n",
            "50%    15857.00000    338.000000       10.000000        409.000000   \n",
            "75%    23785.50000    542.000000       12.000000        713.000000   \n",
            "max    31714.00000    731.000000       23.000000       8474.000000   \n",
            "\n",
            "       n_unique_tokens  n_non_stop_words  n_non_stop_unique_tokens  \\\n",
            "count     31715.000000      31715.000000              31715.000000   \n",
            "mean          0.530754          0.970140                  0.672973   \n",
            "std           0.137106          0.170203                  0.154128   \n",
            "min           0.000000          0.000000                  0.000000   \n",
            "25%           0.471276          1.000000                  0.626028   \n",
            "50%           0.539568          1.000000                  0.690909   \n",
            "75%           0.608523          1.000000                  0.754644   \n",
            "max           1.000000          1.000000                  1.000000   \n",
            "\n",
            "          num_hrefs  num_self_hrefs      num_imgs  ...  min_positive_polarity  \\\n",
            "count  31715.000000    31715.000000  25340.000000  ...           31715.000000   \n",
            "mean      10.865143        3.274381      4.533899  ...               0.095466   \n",
            "std       11.295386        3.822364      8.355645  ...               0.071362   \n",
            "min        0.000000        0.000000      0.000000  ...               0.000000   \n",
            "25%        4.000000        1.000000      1.000000  ...               0.050000   \n",
            "50%        7.000000        3.000000      1.000000  ...               0.100000   \n",
            "75%       14.000000        4.000000      4.000000  ...               0.100000   \n",
            "max      304.000000      116.000000    128.000000  ...               1.000000   \n",
            "\n",
            "       max_positive_polarity  avg_negative_polarity  min_negative_polarity  \\\n",
            "count           31715.000000           31715.000000           31715.000000   \n",
            "mean                0.756271              -0.259198              -0.521164   \n",
            "std                 0.247798               0.127508               0.290352   \n",
            "min                 0.000000              -1.000000              -1.000000   \n",
            "25%                 0.600000              -0.328333              -0.700000   \n",
            "50%                 0.800000              -0.252976              -0.500000   \n",
            "75%                 1.000000              -0.186111              -0.300000   \n",
            "max                 1.000000               0.000000               0.000000   \n",
            "\n",
            "       max_negative_polarity  title_subjectivity  title_sentiment_polarity  \\\n",
            "count           31715.000000        31715.000000              31715.000000   \n",
            "mean               -0.107397            0.281647                  0.069909   \n",
            "std                 0.094932            0.323834                  0.265514   \n",
            "min                -1.000000            0.000000                 -1.000000   \n",
            "25%                -0.125000            0.000000                  0.000000   \n",
            "50%                -0.100000            0.144444                  0.000000   \n",
            "75%                -0.050000            0.500000                  0.141667   \n",
            "max                 0.000000            1.000000                  1.000000   \n",
            "\n",
            "       abs_title_subjectivity  abs_title_sentiment_polarity         shares  \n",
            "count            31715.000000                  31715.000000   31715.000000  \n",
            "mean                 0.341805                      0.155974    3407.068863  \n",
            "std                  0.188980                      0.225957   11899.460219  \n",
            "min                  0.000000                      0.000000       4.000000  \n",
            "25%                  0.166667                      0.000000     946.000000  \n",
            "50%                  0.500000                      0.000000    1400.000000  \n",
            "75%                  0.500000                      0.250000    2800.000000  \n",
            "max                  0.500000                      1.000000  843300.000000  \n",
            "\n",
            "[8 rows x 47 columns]\n"
          ]
        }
      ],
      "source": [
        "# Compute basic statistics\n",
        "print(devp_df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJiTaGSg7xwE"
      },
      "source": [
        "# 1. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection is performed to remove non-predictive features and those that have the lowest importance in the random forest regression algorithm which will be used later.\n",
        "\n",
        "Removing outliers using the IQR methods. Some tuning is necessary in veryfing the range of values to keep, to avoid removing too much data."
      ],
      "metadata": {
        "id": "bQu9b9MN7dNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "devp_df = devp_df.drop([\"id\", \"url\", \"kw_max_max\", \"kw_min_min\",'abs_title_sentiment_polarity', 'max_positive_polarity'], axis = 1)\n",
        "Q1 = devp_df.quantile(0.25, numeric_only=True)\n",
        "Q3 = devp_df.quantile(0.75, numeric_only=True)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# # Align DataFrame and Series before comparing\n",
        "devp_df, IQR = devp_df.align(IQR, axis=1)\n",
        "\n",
        "# # Only keep rows in dataframe that have values within 7*IQR of Q1 and Q3\n",
        "devp_df =devp_df[~((devp_df < (Q1 - 7 * IQR)) | (devp_df > (Q3 + 7 * IQR))).any(axis=1)]\n"
      ],
      "metadata": {
        "id": "EIrq2J5J9sCh"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwMd2i3P7xwE",
        "outputId": "1887f041-d7f8-44ce-fe29-8f91f9523086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(24455, 44)\n"
          ]
        }
      ],
      "source": [
        "# Check the data structure and dimensions\n",
        "print(devp_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qee2QsDb7xwF",
        "outputId": "b4ca1fb9-ed59-493d-f729-c70992fb5adf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'abs_title_subjectivity', 'average_token_length', 'avg_negative_polarity', 'avg_positive_polarity', 'global_rate_negative_words', 'global_rate_positive_words', 'global_sentiment_polarity', 'global_subjectivity', 'kw_avg_avg', 'kw_avg_max', 'kw_avg_min', 'kw_max_avg', 'kw_max_min', 'kw_min_avg', 'kw_min_max', 'max_negative_polarity', 'min_negative_polarity', 'min_positive_polarity', 'n_non_stop_unique_tokens', 'n_non_stop_words', 'n_tokens_content', 'n_tokens_title', 'n_unique_tokens', 'num_hrefs', 'num_imgs', 'num_keywords', 'num_self_hrefs', 'num_videos', 'rate_negative_words', 'rate_positive_words', 'self_reference_avg_sharess', 'self_reference_max_shares', 'self_reference_min_shares', 'shares', 'timedelta', 'title_sentiment_polarity', 'title_subjectivity']\n"
          ]
        }
      ],
      "source": [
        "# Get the names of all columns in the dataset\n",
        "all_columns = devp_df.columns\n",
        "\n",
        "# Identify the numeric columns by excluding any non-numeric columns\n",
        "numeric_columns = [col for col in all_columns if devp_df[col].dtype in ['int64', 'float64']]\n",
        "print (numeric_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the number of missing values and which features contain missing values for both sets separately"
      ],
      "metadata": {
        "id": "vp-edQMnEL2g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGk0ViuM7xwG",
        "outputId": "0c288343-4fbb-4ca3-8e3c-168329bdb585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDA_00                           0\n",
            "LDA_01                           0\n",
            "LDA_02                           0\n",
            "LDA_03                           0\n",
            "LDA_04                           0\n",
            "abs_title_subjectivity           0\n",
            "average_token_length             0\n",
            "avg_negative_polarity            0\n",
            "avg_positive_polarity            0\n",
            "data_channel                     0\n",
            "global_rate_negative_words       0\n",
            "global_rate_positive_words       0\n",
            "global_sentiment_polarity        0\n",
            "global_subjectivity              0\n",
            "kw_avg_avg                       0\n",
            "kw_avg_max                       0\n",
            "kw_avg_min                       0\n",
            "kw_max_avg                       0\n",
            "kw_max_min                       0\n",
            "kw_min_avg                       0\n",
            "kw_min_max                       0\n",
            "max_negative_polarity            0\n",
            "min_negative_polarity            0\n",
            "min_positive_polarity            0\n",
            "n_non_stop_unique_tokens         0\n",
            "n_non_stop_words                 0\n",
            "n_tokens_content                 0\n",
            "n_tokens_title                   0\n",
            "n_unique_tokens                  0\n",
            "num_hrefs                        0\n",
            "num_imgs                      4995\n",
            "num_keywords                  4815\n",
            "num_self_hrefs                   0\n",
            "num_videos                    5017\n",
            "rate_negative_words              0\n",
            "rate_positive_words              0\n",
            "self_reference_avg_sharess       0\n",
            "self_reference_max_shares        0\n",
            "self_reference_min_shares        0\n",
            "shares                           0\n",
            "timedelta                        0\n",
            "title_sentiment_polarity         0\n",
            "title_subjectivity               0\n",
            "weekday                          0\n",
            "dtype: int64\n",
            "id                                 0\n",
            "url                                0\n",
            "timedelta                          0\n",
            "n_tokens_title                     0\n",
            "n_tokens_content                   0\n",
            "n_unique_tokens                    0\n",
            "n_non_stop_words                   0\n",
            "n_non_stop_unique_tokens           0\n",
            "num_hrefs                          0\n",
            "num_self_hrefs                     0\n",
            "num_imgs                        1552\n",
            "num_videos                      1594\n",
            "average_token_length               0\n",
            "num_keywords                    1608\n",
            "kw_min_min                         0\n",
            "kw_max_min                         0\n",
            "kw_avg_min                         0\n",
            "kw_min_max                         0\n",
            "kw_max_max                         0\n",
            "kw_avg_max                         0\n",
            "kw_min_avg                         0\n",
            "kw_max_avg                         0\n",
            "kw_avg_avg                         0\n",
            "self_reference_min_shares          0\n",
            "self_reference_max_shares          0\n",
            "self_reference_avg_sharess         0\n",
            "LDA_00                             0\n",
            "LDA_01                             0\n",
            "LDA_02                             0\n",
            "LDA_03                             0\n",
            "LDA_04                             0\n",
            "global_subjectivity                0\n",
            "global_sentiment_polarity          0\n",
            "global_rate_positive_words         0\n",
            "global_rate_negative_words         0\n",
            "rate_positive_words                0\n",
            "rate_negative_words                0\n",
            "avg_positive_polarity              0\n",
            "min_positive_polarity              0\n",
            "max_positive_polarity              0\n",
            "avg_negative_polarity              0\n",
            "min_negative_polarity              0\n",
            "max_negative_polarity              0\n",
            "title_subjectivity                 0\n",
            "title_sentiment_polarity           0\n",
            "abs_title_subjectivity             0\n",
            "abs_title_sentiment_polarity       0\n",
            "data_channel                       0\n",
            "weekday                            0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values for both sets\n",
        "missing_values = (devp_df.isnull().sum())\n",
        "print (missing_values)\n",
        "\n",
        "missing_values_eval = (eval_df.isnull().sum())\n",
        "\n",
        "print(missing_values_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection on the evaluation set. The 'id' column will be reused later to return the final dataset: ('Id', 'Predicted')"
      ],
      "metadata": {
        "id": "c-sj4tV8DbL2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "dkdmeYnX7xwF"
      },
      "outputs": [],
      "source": [
        "# removing the non-predictive column i.e. url\n",
        "to_remove_columns_eval = [\"id\", \"url\", \"kw_max_max\", \"kw_min_min\",'abs_title_sentiment_polarity', 'max_positive_polarity']\n",
        "#devp_df = devp_df.drop(non_predictive_cols, axis=1)\n",
        "id_col = eval_df['id']\n",
        "eval_df = eval_df.drop(to_remove_columns_eval, axis = 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging the two dataset for preprocessing before splitting them again later for training and testing"
      ],
      "metadata": {
        "id": "OmMY0RkAYpu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remember the number of rows in each dataset\n",
        "n_dev = devp_df.shape[0]\n",
        "n_eval = eval_df.shape[0]\n",
        "\n",
        "# Concatenate the two datasets\n",
        "full_df = pd.concat([devp_df, eval_df])"
      ],
      "metadata": {
        "id": "tPQs3UqMX8uJ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text processing on the 'url' feature (attempted, not used in the model)"
      ],
      "metadata": {
        "id": "1-l7ZuAYQTvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from urllib.parse import urlparse\n",
        "#import datetime\n",
        "#from sklearn.feature_extraction.text import CountVectorizer\n",
        "#from sklearn.feature_extraction.text import TfidfTransformer\n",
        "#from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Parse the URLs to extract the domain and path\n",
        "#full_df['parsed_url'] = full_df['url'].apply(lambda x: urlparse(x))\n",
        "#full_df['domain'] = full_df['parsed_url'].apply(lambda x: x.netloc)\n",
        "#full_df['path'] = full_df['parsed_url'].apply(lambda x: x.path)\n",
        "\n",
        "# Extract the publication year, month, and day from the path\n",
        "#full_df['year'] = full_df['path'].apply(lambda x: x.split('/')[1])\n",
        "#full_df['month'] = full_df['path'].apply(lambda x: x.split('/')[2])\n",
        "#full_df['day'] = full_df['path'].apply(lambda x: x.split('/')[3])\n",
        "\n",
        "# Extract the topic from the path\n",
        "#full_df['topic'] = full_df['path'].apply(lambda x: x.split('/')[4])\n",
        "\n",
        "# Convert the year, month, and day to a datetime object\n",
        "#full_df['date'] = pd.to_datetime(full_df[['year', 'month', 'day']], errors='coerce')\n",
        "\n",
        "# Extract the day of the week from the date\n",
        "#full_df['day_of_week'] = full_df['date'].dt.dayofweek\n",
        "\n",
        "# Drop the original 'url' and 'parsed_url' columns, as well as the 'date' column\n",
        "#full_df = full_df.drop(['url', 'parsed_url', 'date', 'domain', 'day_of_week', 'path', 'year'], axis=1)\n",
        "\n",
        "# Create a pipeline to transform the 'topic' feature\n",
        "#pipeline = Pipeline([\n",
        "#    ('vect', CountVectorizer()),  # Convert the text into a matrix of token counts\n",
        "#    ('tfidf', TfidfTransformer())  # Convert the counts into TF-IDF features\n",
        "#])\n",
        "\n",
        "# Fit the pipeline to the 'topic' feature and transform the data\n",
        "#topic_features = pipeline.fit_transform(full_df['topic'])\n",
        "\n",
        "# Reset the index of full_df\n",
        "#full_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Convert the sparse matrix of features into a DataFrame\n",
        "#topic_features_df = pd.DataFrame(topic_features.toarray(), columns=pipeline.named_steps['vect'].get_feature_names_out())\n",
        "\n",
        "# Concatenate the original DataFrame with the new features\n",
        "#full_df = pd.concat([full_df, topic_features_df], axis=1)\n",
        "\n",
        "# Drop the original 'topic' column\n",
        "#full_df = full_df.drop('topic', axis=1)\n"
      ],
      "metadata": {
        "id": "WRjJk66uQS09"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling missing values"
      ],
      "metadata": {
        "id": "tdZt53ZeEBXZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "l1nIqPeH7xwG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce73fe2a-4cee-4112-d6b1-1a8b8bbbdb6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-f5cb27b279bb>:2: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  full_df = full_df.fillna(full_df.mean())\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Replace missing values in the development set with the mean for each feature\n",
        "full_df = full_df.fillna(full_df.mean(numeric_only = True))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "print(\"Total number of features: \", full_df.shape[1])\n",
        "\n",
        "# Get the data types of each column\n",
        "data_types = full_df.dtypes\n",
        "\n",
        "# Filter columns with object or string data types\n",
        "categorical_cols = data_types[data_types == \"object\"].index.tolist()\n",
        "\n",
        "# Print the number of categorical features\n",
        "print(\"Number of categorical features: \", categorical_cols)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDFw7J5ffBu8",
        "outputId": "80157e32-d7ae-4b45-aca0-c92b7340d6c6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of features:  44\n",
            "Number of categorical features:  ['data_channel', 'weekday']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmsRFDUH7xwH"
      },
      "source": [
        "Separate the target features for the stacked dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "wkIFun387xwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb467164-e198-41b6-f21e-78f391223108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of         LDA_00    LDA_01    LDA_02    LDA_03    LDA_04  \\\n",
            "1     0.020007  0.020008  0.325602  0.020004  0.614379   \n",
            "4     0.214708  0.025062  0.025016  0.025187  0.710028   \n",
            "5     0.020002  0.278532  0.020001  0.661337  0.020127   \n",
            "6     0.040032  0.040014  0.040015  0.040008  0.839932   \n",
            "7     0.025036  0.268234  0.025006  0.155403  0.526322   \n",
            "...        ...       ...       ...       ...       ...   \n",
            "7912  0.033752  0.033453  0.033383  0.033750  0.865662   \n",
            "7913  0.028572  0.529119  0.028572  0.385166  0.028571   \n",
            "7914  0.025001  0.899943  0.025001  0.025055  0.025000   \n",
            "7915  0.025039  0.025068  0.278784  0.646107  0.025003   \n",
            "7916  0.033337  0.531415  0.202306  0.033746  0.199197   \n",
            "\n",
            "      abs_title_subjectivity  average_token_length  avg_negative_polarity  \\\n",
            "1                   0.500000              4.576541              -0.157500   \n",
            "4                   0.300000              5.006993              -0.251786   \n",
            "5                   0.500000              4.316770              -0.200000   \n",
            "6                   0.000000              4.524000              -0.226852   \n",
            "7                   0.250000              4.410909              -0.309568   \n",
            "...                      ...                   ...                    ...   \n",
            "7912                0.500000              4.878049               0.000000   \n",
            "7913                0.194444              4.756881              -0.147917   \n",
            "7914                0.350000              4.691860              -0.104233   \n",
            "7915                0.000000              4.322222              -0.200000   \n",
            "7916                0.500000              5.151724              -0.344444   \n",
            "\n",
            "      avg_positive_polarity   data_channel  ...  num_videos  \\\n",
            "1                  0.419786           tech  ...    1.000000   \n",
            "4                  0.303429           tech  ...    0.000000   \n",
            "5                  0.407727  entertainment  ...    1.000000   \n",
            "6                  0.353333           tech  ...    0.000000   \n",
            "7                  0.440758           tech  ...    1.000000   \n",
            "...                     ...            ...  ...         ...   \n",
            "7912               0.352778      lifestyle  ...    1.000000   \n",
            "7913               0.510606      lifestyle  ...    0.000000   \n",
            "7914               0.357143  entertainment  ...    0.000000   \n",
            "7915               0.386667      lifestyle  ...    0.671713   \n",
            "7916               0.490476  entertainment  ...    0.000000   \n",
            "\n",
            "      rate_negative_words  rate_positive_words  self_reference_avg_sharess  \\\n",
            "1                0.370370             0.629630                    0.000000   \n",
            "4                0.411765             0.588235                  822.000000   \n",
            "5                0.333333             0.666667                  906.000000   \n",
            "6                0.230769             0.769231                  919.000000   \n",
            "7                0.545455             0.454545                16450.000000   \n",
            "...                   ...                  ...                         ...   \n",
            "7912             0.000000             1.000000                    0.000000   \n",
            "7913             0.266667             0.733333                 2400.000000   \n",
            "7914             0.818182             0.181818                 2621.666667   \n",
            "7915             0.166667             0.833333                  952.000000   \n",
            "7916             0.611111             0.388889                  712.000000   \n",
            "\n",
            "      self_reference_max_shares  self_reference_min_shares  timedelta  \\\n",
            "1                           0.0                        0.0      532.0   \n",
            "4                         822.0                      822.0      728.0   \n",
            "5                         915.0                      888.0       38.0   \n",
            "6                         919.0                      919.0      721.0   \n",
            "7                       18800.0                    14100.0      294.0   \n",
            "...                         ...                        ...        ...   \n",
            "7912                        0.0                        0.0      616.0   \n",
            "7913                     2400.0                     2400.0      516.0   \n",
            "7914                     5200.0                      865.0       35.0   \n",
            "7915                      952.0                      952.0      716.0   \n",
            "7916                      712.0                      712.0      427.0   \n",
            "\n",
            "      title_sentiment_polarity  title_subjectivity   weekday  \n",
            "1                        0.000            0.000000  thursday  \n",
            "4                       -0.100            0.200000  thursday  \n",
            "5                        0.000            0.000000    monday  \n",
            "6                        0.500            0.500000  thursday  \n",
            "7                        0.250            0.250000  thursday  \n",
            "...                        ...                 ...       ...  \n",
            "7912                     0.000            0.000000  thursday  \n",
            "7913                     0.375            0.694444  saturday  \n",
            "7914                    -0.050            0.150000  thursday  \n",
            "7915                     0.250            0.500000   tuesday  \n",
            "7916                     0.000            0.000000  thursday  \n",
            "\n",
            "[32372 rows x 43 columns]>\n"
          ]
        }
      ],
      "source": [
        "# Separate the target variable (shares)\n",
        "y_full = full_df[\"shares\"]\n",
        "X_full = full_df.drop(\"shares\", axis=1)\n",
        "\n",
        "print(X_full.head)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljWKZ87W7xwH"
      },
      "source": [
        "Encoding of categorical features with one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "VfUctQ9w7xwI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b133925-7bdb-4a3d-cd1e-b3fa237547ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['data_channel', 'weekday'], dtype='object')\n",
            "Total number of features:  54\n"
          ]
        }
      ],
      "source": [
        "#import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "#X_full['year'] = X_full['year'].astype(int)\n",
        "#X_full['month'] = X_full['month'].astype(int)\n",
        "#X_full['day'] = X_full['day'].astype(int)\n",
        "\n",
        "\n",
        "# Select only the categorical columns\n",
        "categorical_columns = X_full.select_dtypes(include=['object']).columns\n",
        "print(categorical_columns)\n",
        "# Perform one-hot encoding on the categorical columns\n",
        "X_full = pd.get_dummies(X_full, columns=categorical_columns)\n",
        "\n",
        "print(\"Total number of features: \", X_full.shape[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the dataset again"
      ],
      "metadata": {
        "id": "WlxRJFxxaypw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_dev = y_full.iloc[:n_dev]\n",
        "\n",
        "X_dev_processed = X_full.iloc[:n_dev]\n",
        "\n",
        "X_eval_processed = X_full.iloc[n_dev:]\n",
        "\n",
        "print(X_eval_processed.shape)\n",
        "print(X_dev_processed.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtYzVIhhaDjr",
        "outputId": "cd82281e-fb21-4824-ce95-eb656f14c6fc"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7917, 54)\n",
            "(24455, 54)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wapo_c7mzLgB"
      },
      "source": [
        "# 2 Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgECbJe7zWB6",
        "outputId": "5ba76524-b203-4f40-c0ea-1086348af650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "Root Mean Squared Error on Validation set:  2297.8687025597997\n",
            "[('kw_avg_avg', 0.0825497363469677), ('kw_max_avg', 0.03473059391288893), ('average_token_length', 0.03317319289502724), ('kw_avg_max', 0.03230908039719175), ('timedelta', 0.03228941626840139), ('global_subjectivity', 0.031013968533578778), ('kw_avg_min', 0.030302681216337158), ('LDA_00', 0.03018356545398294), ('LDA_01', 0.029592102885157314), ('self_reference_min_shares', 0.02945272636832846), ('LDA_04', 0.029402415094702688), ('global_rate_positive_words', 0.028176054748499314), ('LDA_03', 0.027727243406407316), ('avg_positive_polarity', 0.0273123671610327), ('n_unique_tokens', 0.0271617184551294), ('n_non_stop_unique_tokens', 0.026909558866630608), ('global_sentiment_polarity', 0.02634781993350155), ('LDA_02', 0.025667528967522602), ('self_reference_avg_sharess', 0.024748698719953676), ('num_hrefs', 0.02437358620062374), ('kw_max_min', 0.024329095158616268), ('avg_negative_polarity', 0.023473937430784485), ('global_rate_negative_words', 0.023123766033299083), ('title_sentiment_polarity', 0.021610646961756103), ('kw_min_avg', 0.020260042571761106), ('n_tokens_content', 0.019323477166970922), ('n_tokens_title', 0.019252672979938466), ('n_non_stop_words', 0.017940568650390912), ('self_reference_max_shares', 0.01722939484710044), ('max_negative_polarity', 0.014821603702963758), ('kw_min_max', 0.014346739621198253), ('num_self_hrefs', 0.013951139999316077), ('num_imgs', 0.013672688391246439), ('rate_negative_words', 0.011710247903798997), ('min_positive_polarity', 0.011410261329691189), ('rate_positive_words', 0.011277255441835872), ('title_subjectivity', 0.011262195957480103), ('min_negative_polarity', 0.010285498857699162), ('num_videos', 0.009709537586754404), ('abs_title_subjectivity', 0.009472048130068503), ('num_keywords', 0.008487907132388879), ('weekday_saturday', 0.0060785936476899305), ('weekday_sunday', 0.00576789944779581), ('data_channel_socmed', 0.004790341594662858), ('data_channel_tech', 0.004290600952196759), ('weekday_monday', 0.003015787022316274), ('data_channel_entertainment', 0.002563800025491283), ('weekday_tuesday', 0.0025200426904914168), ('weekday_friday', 0.0022973558590899178), ('weekday_wednesday', 0.0020627750378073577), ('data_channel_lifestyle', 0.002031661934176135), ('weekday_thursday', 0.001728862690653002), ('data_channel_world', 0.0013557708339198308), ('data_channel_bus', 0.00112172657678478)]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# Create the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Split the data into training and validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_dev_processed, y_dev, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the scaler to the training data and transform the training data\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "# Transform the validation data using the same scaler\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "# clusters = dbscan.fit_predict(X_train)\n",
        "\n",
        "# # Identify the core samples\n",
        "# core_samples_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n",
        "# core_samples_mask[dbscan.core_sample_indices_] = True\n",
        "\n",
        "# Remove the outliers from the training data\n",
        "#X_train= X_train[core_samples_mask]\n",
        "#y_train = y_train[core_samples_mask]\n",
        "\n",
        "\n",
        "X_eval_processed = scaler.transform(X_eval_processed)\n",
        "\n",
        "# pca = PCA(n_components=0.95)\n",
        "# X_train = pca.fit_transform(X_train)\n",
        "# X_val = pca.transform(X_val)\n",
        "# X_eval_processed = pca.transform(X_eval_processed)\n",
        "\n",
        "#print(X_train.shape)\n",
        "# Define the model\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define the parameters to tune\n",
        "param_grid = {\n",
        "    #the commented paramters have been used for parameter tuning\n",
        "    'n_estimators': [50], #62 and 100 other parameters used\n",
        "    'max_depth': [None] #10, 20 other parameters used\n",
        "}\n",
        "\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv = 5 , n_jobs=-1, verbose=2) #5 folds have been used turing tuning, but reduced to 2 for speed in the final evaluation\n",
        "\n",
        "# Fit the GridSearchCV object to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Get the best estimator\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_val_pred = best_model.predict(X_val)\n",
        "\n",
        "# Evaluate the model using RMSE on validation set\n",
        "rmse_val = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "print(\"Root Mean Squared Error on Validation set: \", rmse_val)\n",
        "\n",
        "#The random foret regressor we have trained can help us determine how useful each feature is for\n",
        "#the model. We can extract this feature importance and sort them in descending order.\n",
        "#print(sorted(zip(X_dev_processed.columns, best_model.feature_importances_), key=lambda x: x[1],reverse=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
        "import numpy as np\n",
        "\n",
        "# Create the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Create the PolynomialFeatures transformer\n",
        "#poly = PolynomialFeatures(degree=2)\n",
        "\n",
        "# Split the data into training and validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_dev_processed, y_dev, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the scaler to the training data and transform the training data\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "# Transform the training data to polynomial features\n",
        "#X_train = poly.fit_transform(X_train)\n",
        "\n",
        "# Transform the validation data using the same scaler and PolynomialFeatures transformer\n",
        "X_val = scaler.transform(X_val)\n",
        "#X_val = poly.transform(X_val)\n",
        "\n",
        "X_eval_processed = scaler.transform(X_eval_processed)\n",
        "#X_eval_processed = poly.transform(X_eval_processed)\n",
        "\n",
        "# Define the model\n",
        "model = Ridge(random_state=42)\n",
        "\n",
        "# Define the parameters to tune\n",
        "param_grid = {'alpha': np.logspace(-4, 4, 20)}\n",
        "\n",
        "# Create a RandomizedSearchCV object\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv = 5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Get the best estimator\n",
        "best_model_ridge = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_val_pred = best_model_ridge.predict(X_val)\n",
        "\n",
        "# Evaluate the model using RMSE on validation set\n",
        "rmse_val = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "print(\"Root Mean Squared Error on Validation set: \", rmse_val)\n",
        "print(\"Best value of alpha: \", best_params['alpha'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn3L-wagdud_",
        "outputId": "80e73605-7545-4805-fa38-054cb723c7cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Root Mean Squared Error on Validation set:  2250.080846755386\n",
            "Best value of alpha:  206.913808111479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the model to make predictions\n",
        "shares_pred = best_model.predict(X_eval_processed)\n",
        "print(shares_pred.shape)\n",
        "# Add the predicted values as a new 'shares' column in the new data\n",
        "eval_df['Predicted'] = shares_pred\n",
        "eval_df['Id'] = id_col\n",
        "# Assuming new_data is your DataFrame\n",
        "columns_to_keep = ['Id', 'Predicted']\n",
        "result = eval_df[columns_to_keep]\n",
        "\n",
        "# Save the new data with the predicted values to a new CSV file\n",
        "result.to_csv('result.csv', index=False)"
      ],
      "metadata": {
        "id": "axEpcolsMpbC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5c634ae-9a00-4e52-d1a5-fa0ff36ae78b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7917,)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}